package bm.hadoop.join;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileSplit;
import org.apache.hadoop.mapreduce.Mapper;

public class GenericReplicatedJoin extends
		Mapper<Object, Object, Object, Object> {
	private Map<Object, List<Pair>> cachedRecords = new HashMap<Object, List<Pair>>();
	private boolean distributedCacheIsSmaller;
	private Path[] distributedCacheFiles;

	public IDistributedCacheFileReader getDistributedCacheReader() {
		return new TextDistributedCacheFileReader();
	}

	public Pair<Text, Text> join(Pair inputSplitPair, Pair distCachePair) {
		StringBuilder sb = new StringBuilder();
		if (inputSplitPair.getData() != null) {
			sb.append(inputSplitPair.getData());
		}
		sb.append("\t");
		if (distCachePair.getData() != null) {
			sb.append(distCachePair.getData());
		}
		return new Pair<Text, Text>(
				new Text(inputSplitPair.getKey().toString()), new Text(
						sb.toString()));
	}

	public void joinAndCollect(Pair p, Context context) throws IOException,
			InterruptedException {
		List<Pair> cached = cachedRecords.get(p.getKey());
		if (cached != null) {
			for (Pair cp : cached) {
				Pair result;
				if (distributedCacheIsSmaller) {
					result = join(p, cp);
				} else {
					result = join(cp, p);
				}
				if (result != null) {
					context.write(result.getKey(), result.getData());
				}
			}
		}
	}

	@Override
	protected void map(Object key, Object value,
			Mapper<Object, Object, Object, Object>.Context context)
			throws IOException, InterruptedException {
		super.map(key, value, context);

		Pair<String, String> pair = new Pair<String, String>(key.toString(), value.toString());
		if (distributedCacheIsSmaller) {
			joinAndCollect(pair, context);
		} else {
			addToCache(pair);
		}
	}

	@Override
	protected void cleanup(Context context) throws IOException,
			InterruptedException {
		if (!distributedCacheIsSmaller) {
			System.out.println("Outputting in cleanup");

			for (Path distFile : distributedCacheFiles) {
				File distributedCacheFile = new File(distFile.toString());
				IDistributedCacheFileReader reader = getDistributedCacheReader();
				reader.init(distributedCacheFile);
				for (Pair p : (Iterable<Pair>) reader) {
					joinAndCollect(p, context);
				}
				reader.close();
			}
		}
	}

	@SuppressWarnings("deprecation")
	@Override
	protected void setup(Mapper<Object, Object, Object, Object>.Context context)
			throws IOException, InterruptedException {
		super.setup(context);
		distributedCacheFiles = context.getLocalCacheFiles();

		int distCacheSizes = 0;
		for (Path distFile : distributedCacheFiles) {
			File distributedCacheFile = new File(distFile.toString());
			distCacheSizes += distributedCacheFile.length();
		}

		if (context.getInputSplit() instanceof FileSplit) {
			FileSplit split = (FileSplit) context.getInputSplit();

			long inputSplitSize = split.getLength();

			distributedCacheIsSmaller = (distCacheSizes < inputSplitSize);
		} else {
			// if the input split isn't a FileSplit, then assume the
			// distributed cache is smaller than the input split
			//
			distributedCacheIsSmaller = true;
		}

		System.out.println("distributedCacheIsSmaller = "
				+ distributedCacheIsSmaller);

		if (distributedCacheIsSmaller) {
			for (Path distFile : distributedCacheFiles) {
				File distributedCacheFile = new File(distFile.toString());
				IDistributedCacheFileReader reader = getDistributedCacheReader();
				reader.init(distributedCacheFile);
				for (Pair p : (Iterable<Pair>) reader) {
					addToCache(p);
				}
				reader.close();
			}
		}
	}

	private void addToCache(Pair pair) {
		List<Pair> values = cachedRecords.get(pair.getKey());
		if (values == null) {
			values = new ArrayList<Pair>();
			cachedRecords.put(pair.getKey(), values);
		}
		values.add(pair);
	}

}
